{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-oda2/ml-jhu-hepth/blob/master/random_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-G7ZxTWsXLH",
        "colab_type": "code",
        "outputId": "583f6629-b7f8-4cf6-ec4e-80806e5ff466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm1fN5wunpU_",
        "colab_type": "text"
      },
      "source": [
        "(OLD) Generate random (normal) training and testing data and labels. \n",
        "Random NN with 6 layers and 2 outputs  (Jared:  This is too complicated -- just use 1 or 2 layers with large fixed size.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdR32GO6sq2n",
        "colab_type": "code",
        "outputId": "d6fd16ae-9502-4d7c-fa2a-6fb8108e7df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "Nc=16 # Params in the original NN's hidden layer\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\tkeras.layers.Dense(Nc, activation='relu'),\n",
        "\tkeras.layers.Dense(2, activation='softmax')\n",
        "\t])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "X = np.random.rand(10000,28,28)\n",
        "y = model.predict(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWWFQApyn5jy",
        "colab_type": "text"
      },
      "source": [
        "(OLD) Train and evaluate. Result is ~50% as expected since test data is random? (Jared:  Yeah, you shouldn't actually do any training at all here, just initialize the random network.) \n",
        "Generate some more random (normal) data and use the previous NN to predict the labels.  (Jared:  Looks good!  As you say, you do want to feed this into the new smaller model, train it, and see how well it does.)\n",
        "Define a new smaller NN with only 2 layers, ask to fit the previous data for the same number of epochs. It again reaches ~50% on test set.   (Jared:  Test set is irrelevant here.  What is relevant is that the smaller network improved on the loss on the training set with labels generated by the larger model -- so it's learning to imitate.  The next thing is just to scan over small model sizes in a natural, systematic way and plot )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_winCv83WQ97",
        "colab_type": "code",
        "outputId": "1bdcab21-82b8-4689-b7c4-875e878c14c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss = []\n",
        "\n",
        "for i in range(Nc):\n",
        "  print('params in hidden layer:',i)\n",
        "  model2 = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(i, activation='relu'),\n",
        "    keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "  model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "  history = model2.fit(X, y, epochs=10,verbose=1)\n",
        "  loss.append(history.history['loss'][-1])\n",
        "  keras.backend.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params in hidden layer: 0\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.1346 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0633 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 95us/sample - loss: 0.0317 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.0172 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 96us/sample - loss: 0.0101 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 95us/sample - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 94us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 95us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 96us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0016 - acc: 1.0000\n",
            "params in hidden layer: 1\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 108us/sample - loss: 0.1347 - acc: 0.9995\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.0634 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0317 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0172 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0101 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 0.0016 - acc: 1.0000\n",
            "params in hidden layer: 2\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0027 - acc: 0.9994\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 9.6150e-04 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 9.2000e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 8.7290e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 8.2656e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.7473e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.2024e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 6.6324e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 5.9334e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 5.0677e-04 - acc: 1.0000\n",
            "params in hidden layer: 3\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.1355 - acc: 0.9914\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0635 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 0.0318 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.0173 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.0102 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 0.0016 - acc: 1.0000\n",
            "params in hidden layer: 4\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0078 - acc: 0.9918\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 9.4241e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 8.5331e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.8239e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.2524e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 6.5960e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 5.9115e-04 - acc: 1.0000\n",
            "params in hidden layer: 5\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 108us/sample - loss: 0.0024 - acc: 0.9981\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 9.3881e-04 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 8.8302e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 8.1582e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.3824e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 6.3712e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 4.7774e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 2.7855e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 1.3263e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.2595e-05 - acc: 1.0000\n",
            "params in hidden layer: 6\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0072 - acc: 0.9930\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 8.9042e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 7.8232e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 6.9925e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 5.9555e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 4.6299e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 3.1506e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 1.8898e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 1.1164e-04 - acc: 1.0000\n",
            "params in hidden layer: 7\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0019 - acc: 0.9999\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 9.3528e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 8.2080e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 7.1933e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 6.2188e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 5.2203e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 4.0971e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 2.9523e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 1.9640e-04 - acc: 1.0000\n",
            "params in hidden layer: 8\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0036 - acc: 0.9952\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 9.8930e-04 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 9.3292e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 8.7035e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 8.0724e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.3551e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 6.3126e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 4.5719e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 2.5038e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 1.1914e-04 - acc: 1.0000\n",
            "params in hidden layer: 9\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 0.0026 - acc: 0.9996\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 6.9970e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 2.8129e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 1.1123e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 6.0178e-05 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 95us/sample - loss: 4.3966e-05 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 3.9609e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 3.7878e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 3.8543e-05 - acc: 1.0000\n",
            "params in hidden layer: 10\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 6.6860e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 3.3634e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 1.3245e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 6.3640e-05 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 4.7616e-05 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 3.6852e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 4.4534e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 3.8312e-05 - acc: 1.0000\n",
            "params in hidden layer: 11\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0083 - acc: 0.9914\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 9.2298e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 8.2562e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.4145e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 6.6350e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 108us/sample - loss: 5.6512e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 4.4707e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 3.1073e-04 - acc: 1.0000\n",
            "params in hidden layer: 12\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 110us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.8504e-04 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 2.5641e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 8.0012e-05 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 5.1918e-05 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 4.5676e-05 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 5.8984e-05 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 5.8195e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 5.1715e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 6.1051e-05 - acc: 1.0000\n",
            "params in hidden layer: 13\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 0.0032 - acc: 0.9988\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 8.4597e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 6.5984e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 4.0025e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 1.6830e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 7.9853e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 4.9980e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 3.9317e-05 - acc: 1.0000\n",
            "params in hidden layer: 14\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 7.3290e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 5.2947e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 3.4891e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 2.0975e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 1.2309e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 7.1311e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 4.8873e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 4.0806e-05 - acc: 1.0000\n",
            "params in hidden layer: 15\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0062 - acc: 0.9971\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 9.4313e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: 8.4066e-04 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.1718e-04 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 4.4116e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 1.6987e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 7.2486e-05 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 4.8228e-05 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: 4.2309e-05 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex4yW-oWrz6o",
        "colab_type": "code",
        "outputId": "41995411-9cdc-4891-d39b-e6c41472bf39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(Nc),loss)\n",
        "plt.title('Losses after 10 epochs. Nc ='+str(Nc))\n",
        "plt.xlabel('# of parameters N in small network ')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOZcSnlCcc86lxP8Hcp3BJZizx7kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZtzhBka6ZEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}